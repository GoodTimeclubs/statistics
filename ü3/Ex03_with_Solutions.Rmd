---
title: "Ex03: Probability Theory — Practice Set II (10 Exercises)"
author: "Martin Simon"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 5, fig.retina = 2)
set.seed(42)
N <- 1e6        # main Monte Carlo sample size (adjust if knitting is slow)
```

## 1) Exercise 2.3.3 — Product of two fair dice (pmf via simulation)

Two fair dice are thrown; let $Y$ be the product. Estimate the pmf of $Y$ by simulation.

```{r}
d1 <- sample(1:6, N, replace = TRUE)
d2 <- sample(1:6, N, replace = TRUE)
Y  <- d1 * d2
pmf_hat <- sort(prop.table(table(Y)))
pmf_tbl <- data.frame(y = as.integer(names(pmf_hat)), pmf_est = as.numeric(pmf_hat))
head(pmf_tbl, 10)
tail(pmf_tbl, 10)
```

## 2) Exercise 2.3.4 — $Z\sim\mathrm{Geometric}(p)$: $\mathbb{P}(5\le Z\le 9)$ via simulation

We adopt the convention $Z\in\{1,2,\dots\}$ (number of trials until the first success). Set $p$ below as needed.

```{r}
p_geo <- 0.3   # <-- change if desired
# simulate geometric: number of Bernoulli(p) trials until first success
# R's rgeom returns failures before first success; add 1 to get trials count.
Z <- rgeom(N, prob = p_geo) + 1L
prob_hat <- mean(Z >= 5 & Z <= 9)
se_hat   <- sqrt(prob_hat * (1 - prob_hat) / N)
c(`p` = p_geo, `P(5<=Z<=9)` = prob_hat, SE = se_hat)
```

## 3) Exercise 2.3.5 — $X\sim\mathrm{Binomial}(12,p)$: maximize $\mathbb{P}(X=11)$ via simulation

We scan a grid of $p$ values, estimate $\mathbb{P}(X=11)$ for each by simulation, and report the maximizer.

```{r}
set.seed(43)
grid <- seq(0, 1, by = 0.005)
nrep <- 20000                             # per-grid Monte Carlo size
p11_est <- numeric(length(grid))

for (i in seq_along(grid)) {
  x <- rbinom(nrep, size = 12, prob = grid[i])
  p11_est[i] <- mean(x == 11)
}

imax <- which.max(p11_est)
data.frame(p_grid = grid[imax], P_Xeq11_est = p11_est[imax])
```

(Optional) You can visualize the estimated curve:

```{r, fig.height=4}
op <- par(mar = c(4, 4, 1, 1)); on.exit(par(op))
plot(grid, p11_est, type = "l", xlab = "p", ylab = "Estimated P(X=11)",
     main = "Binomial(12,p): Monte Carlo estimate of P(X=11)")
abline(v = grid[imax], lty = 2)
```

## 4) Exercise 2.4.3 — $U\sim\mathrm{Uniform}[0,1]$ probabilities via simulation

```{r}
U <- runif(N)
p1 <- mean(U <= 0.5)
p2 <- mean(U >= 1/3 & U <= 2/3)
p3 <- mean(U == 1)  # should be ~0 in simulation as well
c(`P(U<=1/2)` = p1, `P(U in [1/3,2/3])` = p2, `P(U=1)` = p3)
```

## 5) Exercise 2.4.4 — $Z\sim\mathrm{Exponential}(4)$: tail probabilities via simulation

```{r}
lambda <- 4
Z <- rexp(N, rate = lambda)
p_gt5 <- mean(Z > 5)
p_2to5 <- mean(Z > 2 & Z <= 5)
se_gt5 <- sqrt(p_gt5*(1-p_gt5)/N)
se_2to5 <- sqrt(p_2to5*(1-p_2to5)/N)
c(`P(Z>5)` = p_gt5, SE_gt5 = se_gt5, `P(2<Z<=5)` = p_2to5, SE_2to5 = se_2to5)
```

## 6) Exercise 2.4.5 — Find $c$ so that $f$ is a valid density

We estimate $c$ by enforcing $\int_0^1 f(x)\,dx = 1$ using simulation.

(i) $f(x)=c\,x$ on $(0,1)$.\
(ii) $f(x)=c\,x^n$ on $(0,1)$ for a specified $n>-1$.

```{r}
# (i) f(x)=c x
x <- runif(N)                  # MC points on (0,1)
int_hat <- mean(x)             # estimate of ∫_0^1 x dx
c_hat_i <- 1 / int_hat
c_hat_i

# (ii) f(x)=c x^n
n_power <- 2                   # <-- change n here (must be > -1)
if (n_power <= -1) {
  stop("For simulation-based normalization we require n > -1 to ensure integrability.")
}
x <- runif(N)
int_hat_n <- mean(x^n_power)
c_hat_ii <- 1 / int_hat_n
c_hat_ii
```

## 7) Exercise 2.5.3 — CDF of a scaled die via simulation

Let $X$ be the outcome of a fair die scaled by $1/6$, so $X\in\{1/6,\dots,1\}$. Estimate and plot the empirical cdf.

```{r, fig.height=4}
op <- par(mar = c(4, 4, 1, 1)); on.exit(par(op))
die <- sample(1:6, N, replace = TRUE)
X <- die / 6
plot(ecdf(X), main = "Empirical CDF of X = (die outcome)/6",
     xlab = "x", ylab = expression(F[X](x)))
abline(h = seq(0,1,by=1/6), lty = 3)
abline(v = (1:6)/6, lty = 3)
```

We verify cdf properties empirically: nondecreasing (by construction), right-continuous (step ecdf), limits near 0 and 1:

```{r}
range_ecdf <- c(min(X), max(X))
c(min_X = range_ecdf[1], max_X = range_ecdf[2], F_at_0 = mean(X <= 0), F_at_1 = mean(X <= 1))
```

## 8) Exercise 2.5.4 — $X\sim\mathcal{N}(0,1)$: probabilities via simulation

```{r}
Xn <- rnorm(N, mean = 0, sd = 1)
p_a <- mean(Xn > 2 & Xn < 7)
p_b <- mean(Xn <= -3)
se_a <- sqrt(p_a*(1-p_a)/N)
se_b <- sqrt(p_b*(1-p_b)/N)
c(`P(2<X<7)` = p_a, SE_a = se_a, `P(X<=-3)` = p_b, SE_b = se_b)
```

## 9) Exercise 2.6.2 — Affine transform of a uniform via simulation

If $X\sim \mathrm{Uniform}[L,R]$ and $Y=cX+d$ with $c>0$, verify empirically that $Y$ is uniform on $[cL+d,\,cR+d]$.

```{r, fig.height=4}
op <- par(mar = c(4, 4, 1, 1)); on.exit(par(op))
L <- -2; R <- 5; c_coef <- 1.5; d_coef <- -0.25   # parameters (feel free to change)
Xu <- runif(N, min = L, max = R)
Yt <- c_coef * Xu + d_coef
# Compare histogram to theoretical support
hist(Yt, breaks = 60, probability = TRUE, main = "Transformed Uniform: Y = cX + d",
     xlab = "y")
abline(v = c(c_coef*L + d_coef, c_coef*R + d_coef), lty = 2)
# Empirical check of bounds and flatness: simple KS test against Uniform[a,b]
a <- c_coef*L + d_coef; b <- c_coef*R + d_coef
ks <- suppressWarnings(ks.test((Yt - a)/(b - a), "punif"))
c(a = a, b = b, KS_p_value = ks$p.value)
```

## 10) Exercise 2.6.3 — Affine transform of a normal via simulation

If $X\sim \mathcal{N}(\mu,\sigma^2)$ and $Y=cX+d$, verify the transformed moments empirically.

```{r, fig.height=4}
op <- par(mar = c(4, 4, 1, 1)); on.exit(par(op))
mu <- 1.2; sigma <- 0.7; c_coef <- -2; d_coef <- 0.3
Xg <- rnorm(N, mean = mu, sd = sigma)
Yg <- c_coef * Xg + d_coef
c(sample_mean_Y = mean(Yg), sample_var_Y = var(Yg))
hist(Yg, breaks = 60, probability = TRUE, main = "Histogram of Y = cX + d (Normal case)",
     xlab = "y")
```
